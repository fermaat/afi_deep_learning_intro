{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "On this notebook we will take a look at three approaches for hyperparameter optimization, will compare them and build differnt models.\n",
    "\n",
    "## Dataset\n",
    "We will use our old friend MNIST for its simplicity. \n",
    "\n",
    "<font color=red><b>Load the dataset and preprocess it. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import wrangle as wr\n",
    "from numpy import nan\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.keras.backend.clear_session() \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# the data, split between train and test sets\n",
    "...\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "As we will have to create it more than once, Let's build a function for creating the model. The function will recieve the next parameters :\n",
    "- first_neuron\n",
    "- activation\n",
    "- kernel_initializer\n",
    "- dropout_rate\n",
    "- optimizer\n",
    "\n",
    "And the model will consist in :\n",
    "- A dense layer with first unit units, activation activated and initialized with the kernel initializer\n",
    "- A dropout layer with the dropout rate\n",
    "- A dense layer with the number of classes as  the amount of units, softmax activated and initialized with the kernel initializer\n",
    "- Use the given optimizer and categorical crossentropy as the loss function. Add accuracy to the metrics\n",
    "\n",
    "<font color=red><b> Build the model function\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(first_neuron=9,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='uniform',\n",
    "                 dropout_rate=0,\n",
    "                 optimizer='Adam'):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the model using the kerasclassifier wrapper that allows us to introduce the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Create the model\n",
    "model = KerasClassifier(build_fn=create_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "The first technique we are going to use is the brute force: the grid search. Here's the workflow:\n",
    "\n",
    "- Define a grid on n dimensions, where each of these maps for an hyperparameter. e.g. n = (learning_rate, dropout_rate, batch_size)\n",
    "- For each dimension, define the range of possible values: e.g. batch_size = [4, 8, 16, 32, 64, 128, 256]\n",
    "- Search for all the possible configurations and wait for the results to establish the best one: e.g. C1 = (0.1, 0.3, 4) -> acc = 92%, C2 = (0.1, 0.35, 4) -> acc = 92.3%, etc...\n",
    "\n",
    "The real pain point of this approach is known as the curse of dimensionality. This means that more dimensions we add, the more the search will explode in time complexity (usually by an exponential factor), ultimately making this strategy unfeasible!\n",
    "\n",
    "<font color=red><b>Define the range of values you want to experiment with. Don't go crazy with many values because this is computationaly intensive. Finally, create a dictionary with all the components\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Design Components\n",
    "...\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = [10] \n",
    "batch_size = [1024]\n",
    "dropout_rate = [0.0]\n",
    "\n",
    "# Prepare the Grid\n",
    "param_grid = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the grid search.\n",
    "\n",
    "<font color=red><b> Generate the grid and fit it. use 1 job, 3 cf folds and set verbose to 2.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Search!\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search\n",
    "This is our second strategy. The only real difference between Grid Search and Random Search is on the step 1 of the strategy cycle. Random Search picks the point randomly from the configuration space. Now the parameters can vary in a longer grid.In the Grid Layout, it's easy to notice that, even if we have trained 9 models, we have used only 3 values per variable! Whereas, with the Random Layout, it's extremely unlikely that we will select the same variables more than once. It ends up that, with the second approach, we will have trained 9 model using 9 different values for each variables.\n",
    "\n",
    "It is not guaranteed to fnd the best hyperparams, but it is very good on high spaces and gives better results on less iterations.\n",
    "\n",
    "\n",
    "<font color=red><b>Define the distribution of values you want to experiment with. Finally, create a dictionary with all the components\n",
    "    <br> Hint: use randint for integers in a range or uniform for continuous values\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "# Model Design Components\n",
    "...\n",
    "# Hyperparameters\n",
    "epochs = [10] \n",
    "batch_size = [1024] \n",
    "...\n",
    "\n",
    "\n",
    "# Prepare for the Search\n",
    "param_dist = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the random search.\n",
    "\n",
    "<font color=red><b> Generate the randomized search and fit it. use 1 job, 3 cf folds and set verbose to 2. use a max iterations value of 8\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Search!\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# run randomized search\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Show results\n",
    "print(\"Best: %f using %s\" % (random_search.best_score_, random_search.best_params_))\n",
    "means = random_search.cv_results_['mean_test_score']\n",
    "stds = random_search.cv_results_['std_test_score']\n",
    "params = random_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "This search strategy builds a surrogate model that tries to predict the metrics we care about from the hyperparameters configuration.\n",
    "\n",
    "Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point. \n",
    "\n",
    "\n",
    "1. Build a model\n",
    "\n",
    "**Iterate and generate a better estimation of P(validation_metric|hyperparameters):**\n",
    "2. Select Hyperparameters\n",
    "3. Training/Evaluate\n",
    "4. Update the model\n",
    "\n",
    "It is not as expensive as the other two strategies and performs a driven strategy to find the right parameters.\n",
    "\n",
    "\n",
    "\n",
    "<font color=red><b> This strategy needs a function to define the data. Build it:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \n",
    "    ...\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build now the function for creating the model. This is special on this case, as it neds double curly brackets dropped-in as needed. Its return value has to be a valid python dictionary with the next keys:\n",
    "- loss: Specify a numeric evaluation metric to be minimized\n",
    "- status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "- model: specify the model just created so that we can later use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # L1\n",
    "    model.add(Dense({{choice([8,9,10])}}, \n",
    "                    input_shape=(784,),\n",
    "                    kernel_initializer={{choice(['uniform', 'normal'])}}, \n",
    "                    activation={{choice(['relu', 'elu'])}}))\n",
    "    # Dropout\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    # L2\n",
    "    model.add(Dense(10, \n",
    "                    kernel_initializer={{choice(['uniform', 'normal'])}}, \n",
    "                    activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer={{choice(['nadam', 'adam', 'sgd'])}}, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=1024,\n",
    "              epochs=10,\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b> Run 5 iterations using using the Tree Parzen Estimator or TPE algorithm provided with hyperopt.\n",
    "    <br> Hint: use the minimize functon from the optim class in hyperas and the tpe.suggest algorithm\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperas import optim\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_base",
   "language": "python",
   "name": "dl_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
