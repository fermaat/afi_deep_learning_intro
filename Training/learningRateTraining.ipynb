{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate study\n",
    "\n",
    "Deep learning neural networks are trained using the stochastic gradient descent optimization algorithm. The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "\n",
    "In this notebook, we will cover the effects of the learning rate, momentum and learning rate schedules\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "We will use a self-made data\n",
    "\n",
    "Multi-Class Classification Problem\n",
    "\n",
    "We will use a small multi-class classification problem as the basis to demonstrate the effect of learning rate on model performance.\n",
    "\n",
    "The scikit-learn class provides the make_blobs() function that can be used to create a multi-class classification problem with the prescribed number of samples, input variables, classes, and variance of samples within a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Generate a dataset composed of 1000 samples with 3 classes and 2 features. Plot the result\n",
    "<br>Hint: use the blobs kmaker from sklearn and the given parameters</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cluster_std = 2 # force classes to be not linearly separable\n",
    "random_state = 2 \n",
    "\n",
    "# generate 2d classification dataset\n",
    "X, y = ...\n",
    "# scatter plot for each class value\n",
    "for class_value in range(3):\n",
    "    # scatter plot for points with a different color\n",
    "    plt.scatter(...)\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Learning Rate\n",
    "\n",
    "In this section, we will develop a  model to address the blobs classification problem and investigate the effect of different learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Generate a function for creating training and testing datasets. Remember the labels must be in a categorical format\n",
    "<br>Hint: use the imported functions </b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# prepare train and test dataset\n",
    "def prepare_data():\n",
    "    # generate 2d classification dataset\n",
    "    X, y = ..\n",
    "    # one hot encode output variable\n",
    "    y = ...\n",
    "    # split into train and test. Is order ok?\n",
    "    n_train = 500\n",
    "    trainX, testX = ...\n",
    "    trainy, testy = ...\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # For easy reset of notebook state.\n",
    "# study of learning rate on accuracy for blobs problem\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Create a function for generating a model with a the following configuration:\n",
    "    - dense layer with 50 units, relu-activated. \n",
    "    - dense layer with the categories amount as units, softmax activated.\n",
    "<br>use stochastic gradient descent as the optimizer andcompile using categorical crossentropy and accuracy as the metric. Define the fit as a history for plotting </b>\n",
    "<br>Hint: use the imported functions </b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and plot learning curve\n",
    "def fit_model(trainX, trainy, testX, testy, lrate):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    ...\n",
    "    # compile model\n",
    "    ...\n",
    "    # fit model\n",
    "    history = ...\n",
    "    # plot learning curves\n",
    "    pyplot.plot(history.history['accuracy'], label='train')\n",
    "    pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "    pyplot.title('lrate='+str(lrate), pad=-50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b> Prepare the data and train on different learning rates. Show the results and discuss about them\n",
    "<br>Hint: use the functions you just built </b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "trainX, trainy, testX, testy = ...\n",
    "# create learning curves for different learning rates\n",
    "learning_rates = [1E-0, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7]\n",
    "for i in range(len(learning_rates)):\n",
    "    # determine the plot number\n",
    "    plot_no = 420 + (i+1)\n",
    "    plt.subplot(plot_no)\n",
    "    # fit model and plot learning curves for a learning rate\n",
    "    ...\n",
    "# show learning curves\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Dynamics\n",
    "\n",
    "Momentum can smooth the progression of the learning algorithm that, in turn, can accelerate the training process.\n",
    "\n",
    "We will choose the learning rate of 0.01 that in the previous section converged to a reasonable solution, but required more epochs than the learning rate of 0.1\n",
    "\n",
    "<font color=red><b> Define the same model architecture, but this time, modify the momentum for the learner. plot the results\n",
    "<br>Hint: Add the momentum to the optimizer definition</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and plot learning curve\n",
    "def fit_model_momentum(trainX, trainy, testX, testy, momentum):\n",
    "    # define model\n",
    "    ...\n",
    "    # compile model\n",
    "    ...\n",
    "    # fit model\n",
    "    history = ...\n",
    "    # plot learning curves\n",
    "    ...\n",
    "    plt.title('momentum='+str(momentum), pad=-80) \n",
    "\n",
    "\n",
    "momentums = [0.0, 0.5, 0.9, 0.99]\n",
    "for i in range(len(momentums)):\n",
    "    # determine the plot number\n",
    "    plot_no = 220 + (i+1)\n",
    "    plt.subplot(plot_no)\n",
    "    # fit model and plot learning curves for a momentum\n",
    "    ...\n",
    "# show learning curves\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Learning Rate Schedules\n",
    "\n",
    "We will look at two learning rate schedules in this section.\n",
    "\n",
    "The first is the decay built into the SGD class and the second is the ReduceLROnPlateau callback.\n",
    "## Learning Rate Decay\n",
    "\n",
    "We will see now the effects of adding a decay effect on the learning rate. This will make the lr to be smaller the more the network learns. \n",
    "\n",
    "$$lr = \\frac{lr_0}{1+ decay\\cdot t} $$\n",
    "\n",
    "Where *t* is the iteration number and decay is que parameter we add.\n",
    "\n",
    "<font color=red><b> Define the same model architecture, but this time,ad a learning rate decay. Plot the results\n",
    "<br>Hint: Add the decay to the optimizer definition</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model and plot learning curve\n",
    "def fit_model_decay(trainX, trainy, testX, testy, decay):\n",
    "    # define model\n",
    "    ...\n",
    "    # compile model\n",
    "    ...\n",
    "    # fit model\n",
    "    history = ...\n",
    "    # plot learning curves\n",
    "    ...\n",
    "    plt.title('decay='+str(decay), pad=-80)\n",
    "\n",
    "# prepare dataset\n",
    "trainX, trainy, testX, testy = prepare_data()\n",
    "# create learning curves for different decay rates\n",
    "decay_rates = [1E-1, 1E-2, 1E-3, 1E-4]\n",
    "for i in range(len(decay_rates)):\n",
    "    # determine the plot number\n",
    "    plot_no = 220 + (i+1)\n",
    "    plt.subplot(plot_no)\n",
    "    # fit model and plot learning curves for a decay rate\n",
    "    ...\n",
    "    # show learning curves\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_base",
   "language": "python",
   "name": "dl_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
