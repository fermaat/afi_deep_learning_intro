{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST one-to-one: DL Vs Classic ML\n",
    "\n",
    "Interesting test. Who will be the best at it? On the right corner we have DL, which will use only a very simple model, no convolutions, no strange normalization. Nothing. On the left side we have our old friends, SVM, RF, PCA and so on, ready to battle. Let's see what happens.\n",
    "\n",
    "## The dataset\n",
    "You know, It's MNIST\n",
    "\n",
    "\n",
    "<font color=red><b> Import the dataset\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from numpy import expand_dims\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.keras.backend.clear_session() \n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 1: DL\n",
    "\n",
    "As mentioned above, we will make a very simplisticDL model\n",
    "\n",
    "<font color=red><b> Build your own model and train it.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 1s 23us/sample - loss: 0.5540 - accuracy: 0.8439 - val_loss: 0.2379 - val_accuracy: 0.9320\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2146 - accuracy: 0.9377 - val_loss: 0.1783 - val_accuracy: 0.9491\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1697 - accuracy: 0.9506 - val_loss: 0.1547 - val_accuracy: 0.9546\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1477 - accuracy: 0.9566 - val_loss: 0.1358 - val_accuracy: 0.9615\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1330 - accuracy: 0.9609 - val_loss: 0.1282 - val_accuracy: 0.9630\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1242 - accuracy: 0.9632 - val_loss: 0.1264 - val_accuracy: 0.9638\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1168 - accuracy: 0.9653 - val_loss: 0.1198 - val_accuracy: 0.9667\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1107 - accuracy: 0.9671 - val_loss: 0.1239 - val_accuracy: 0.9638\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.1066 - accuracy: 0.9685 - val_loss: 0.1201 - val_accuracy: 0.9652\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.1031 - accuracy: 0.9690 - val_loss: 0.1165 - val_accuracy: 0.9656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f41a06386a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dl = x_train.astype('float32')\n",
    "x_test_dl = x_test.astype('float32')\n",
    "x_train_dl /= 255\n",
    "x_test_dl /= 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_train_binary = to_categorical(y_train)\n",
    "y_test_binary = to_categorical(y_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add (Dense(128, activation='relu', input_shape =(28, 28)))\n",
    "model.add (Dense(32, activation='relu', input_shape =(28, 28)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add (Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train_dl, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_dl, y_test_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 2: Classic baseline\n",
    "\n",
    "Let's see what can the classic guys do with not a big effort:\n",
    "\n",
    "<font color=red><b> Build a couple of classic models and see how it goes. How long does it take?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9255"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_train_classic = x_train.reshape(x_train.shape[0],-1)\n",
    "x_test_classic = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "## Training \n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_classic, y_train)\n",
    "\n",
    "## Predicting\n",
    "y_pred_lr = lr.predict(x_test_classic)\n",
    "logistic_regression_score = accuracy_score(y_test, y_pred_lr)\n",
    "logistic_regression_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9704"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train_classic, y_train)\n",
    "\n",
    "## Predicting\n",
    "y_pred_rf = rf.predict(x_test_classic)\n",
    "\n",
    "random_forest_score = accuracy_score(y_test, y_pred_rf)\n",
    "random_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 3: lets get convolutional\n",
    "Now DL will still being simplistic, but in this case, let's use some convolutions\n",
    "\n",
    "<font color=red><b> Build a simple CNN and see if you can beat classic ML\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.3291 - accuracy: 0.9039 - val_loss: 0.1386 - val_accuracy: 0.9585\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.1289 - accuracy: 0.9628 - val_loss: 0.0833 - val_accuracy: 0.9742\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.0853 - accuracy: 0.9747 - val_loss: 0.0659 - val_accuracy: 0.9808\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0648 - accuracy: 0.9806 - val_loss: 0.0594 - val_accuracy: 0.9805\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0527 - accuracy: 0.9840 - val_loss: 0.0504 - val_accuracy: 0.9833\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.0425 - accuracy: 0.9869 - val_loss: 0.0540 - val_accuracy: 0.9822\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0366 - accuracy: 0.9886 - val_loss: 0.0496 - val_accuracy: 0.9821\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 25us/sample - loss: 0.0299 - accuracy: 0.9907 - val_loss: 0.0469 - val_accuracy: 0.9845\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 25us/sample - loss: 0.0259 - accuracy: 0.9920 - val_loss: 0.0467 - val_accuracy: 0.9867\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.0227 - accuracy: 0.9927 - val_loss: 0.0450 - val_accuracy: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f418ca62cc0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D\n",
    "\n",
    "\n",
    "x_train_dl_conv = x_train_dl.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test_dl_conv = x_test_dl.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_dl_conv, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test_dl_conv, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round 4: The Empire Strikes Back\n",
    "Let's reduce dimensionality so that we can be sure we just use the interesting information\n",
    "\n",
    "<font color=red><b> Use PCA and reduce the dataset dimensions to something, let's say 95% of variability. Then, train again on this new data. Add KNN to the equation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit_transform(x_train_classic)\n",
    "\n",
    "# Calculating optimal k to have 95% (say) variance \n",
    "\n",
    "k = 0\n",
    "total = sum(pca.explained_variance_)\n",
    "current_sum = 0\n",
    "\n",
    "while(current_sum / total < 0.95):\n",
    "    current_sum += pca.explained_variance_[k]\n",
    "    k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=k, whiten=True)\n",
    "\n",
    "x_train_pca = pca.fit_transform(x_train_classic)\n",
    "x_test_pca = pca.transform(x_test_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fer/data/venvs/dl_tf2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9246"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training \n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_pca, y_train)\n",
    "\n",
    "## Predicting\n",
    "y_pred_lr = lr.predict(x_test_pca)\n",
    "logistic_regression_score = accuracy_score(y_test, y_pred_lr)\n",
    "logistic_regression_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9491"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train_pca, y_train)\n",
    "\n",
    "## Predicting\n",
    "y_pred_rf = rf.predict(x_test_pca)\n",
    "\n",
    "random_forest_score = accuracy_score(y_test, y_pred_rf)\n",
    "random_forest_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9017"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "## Training \n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train_pca, y_train)\n",
    "## Predicting\n",
    "y_pred_knn = knn.predict(x_test_pca)\n",
    "\n",
    "knn_score = accuracy_score(y_test, y_pred_knn)\n",
    "knn_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Round: SVM and RF grid search\n",
    "\n",
    "These guys are now taking it seriously. \n",
    "\n",
    "<font color=red><b> Let's see what SVM and a grid search on RF can do. Maybe it is a good idea to train on SVM with even less features... or maybe let the hard training to be at home.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  18 | elapsed:  1.9min remaining:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9639\n",
      "{'max_depth': 28, 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [7, 14, 28],\n",
    "              'n_estimators': [100, 200, 400]}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "gs = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='accuracy', cv=2, n_jobs=-1, verbose=1)\n",
    "gs = gs.fit(x_train_classic, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9255"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=0.1, kernel='rbf', gamma=0.1)\n",
    "clf = clf.fit(x_train_pca, y_train)\n",
    "y_pred_svc = clf.predict(x_test_pca)\n",
    "svm_score = accuracy_score(y_test, y_pred_lr)\n",
    "svm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tf2",
   "language": "python",
   "name": "dl_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
